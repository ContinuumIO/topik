Introduction Tutorial
=====================

In this tutorial we will examine `topik` with a practical example: Topic
Modeling for Movie Reviews.

- The Movie Review Dataset
- Using the high-level interface ``run_topic_model``
- Creating your own custom topic modeling flow
- Analyzing the results


The Movie Review Dataset
------------------------

In this tutorial we are going to use the `Sentiment Polarity Dataset Version 2.0
<http://www.cs.cornell.edu/people/pabo/movie-review-data/>`_ from Bo Pang and
Lillian Lee. This dataset is distributed with `NLTK <http://www.nltk.org/>`_
with permission from the authors.

You can download the individual dataset from `NLTK
<http://www.nltk.org/nltk_data/packages/corpora/movie_reviews.zip>`_, or
download all of ntlk's dataset, running the following commands from the python
interpreter:

.. code-block:: python

   >>> import nltk
   >>> nltk.download()


For more information on the datasets and download options visit `NLTK data
<http://www.nltk.org/data.html>`_.

Instead of using the dataset in for `sentiment analysis`, its initial purpose,
we'll perform `topic modeling` on the movie reviews. For that reason, we'll
merge both folders `pos` and `neg`, to one named `reviews`.


High-level interfaces
---------------------


For quick, one-off studies, the command line interface allows you to specify
minimal information and obtain topic model plot output.


Custom topic modeling flow
--------------------------


For interactive exploration and more efficient, involved workflows, there also
exists a Python API for using each part of the topic modeling workflow. There
are three phases to topic modeling with topik: data import,
tokenization/vectorization, and modeling. Each phase is modular, with several
options available to you for each step.


Data Import
~~~~~~~~~~~

Data import loads your data from some external representation into an iterable,
internal representation for Topik. The main front end for importing data is the
read_input function:


.. code-block:: python

   >>> from topik import read_input
   >>> corpus = read_input(source="data_file.json", content_field="text")


``read_input`` is a front-end to several potential reader backends. Presently,
``read_input`` attempts to recognize which backend to use based on some
characteristics of the source string you pass in. These criteria are:

  * ends with .js or .json: treat as JSON stream filename first, fall back to
    "large JSON" (such as file generated by esdump).
  * contains 8983: treat as solr connection address (8983 is the default solr
    port).
  * contains 9200: treat as Elasticsearch connection address (9200 is the
    default Elasticsearch port).
  * result of os.path.splitext(source)[1] is "": treat as folder of files. Each
    file is considered raw text, and its contents are stored under the key given
    by content_field. Files may be gzipped.

Any of the backends can also be forced by passing the source_type argument with
one of the following string arguments:

  * solr
  * elastic
  * json_stream
  * large_json
  * folder

The ``content_field`` is a mandatory argument that in most cases specifies where the
actual content to be analyzed will be drawn from. For all hierarchical data
sources (everything except folders), this accesses some subfield of the data you
feed in.


JSON additional options
^^^^^^^^^^^^^^^^^^^^^^^

For JSON stream and "large JSON" inputs, an additional keyword argument may be
passed, ``json_prefix``, which is the period-separated path leading to the single
content field. This is for content fields not at the root level of the JSON
document. For example, given the JSON content:

.. code-block:: javascript

    [ {"nested": {"dictionary": {"text": "I am the text you're looking for."} } } ]


You would read using the following ``json_prefix`` argument:


.. code-block:: python

   >>> corpus = read_input(source="data_file.json", content_field="text",
                           json_prefix="nested.dictionary")


Elasticsearch additional options and notes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The Elasticsearch importer expects a full string specifying the Elasticsearch
server. This string at a minimum must contain both the server address and the
index to access (if any). All results returned from the Elasticsearch query
contain only the contents of the '_source' field returned from the query.

.. code-block:: python

   >>> corpus = read_input(source="https://localhost/test_index", content_field="text")


Extra arguments passed by keyword are passed to the Elasticsearch instance
creation. This can be used to pass additional login parameters, for example, to
use SSL:

.. code-block:: python

   >>> corpus = read_input(source="https://user:secret@localhost:443/test_index",
                           content_field="text", use_ssl=True)


The source argument for Elasticsearch also supports multiple servers, though
this requires that you manually specify the 'elastic' source_type:

.. code-block:: python

    >>> corpus = read_input(source=["https://server1", "https://server2"],
                            content_field="text")


For more information on server options, please refer to `Elasticsearch's
documentation <https://elasticsearch-py.readthedocs.org/en/master/>`_.

Extra keyword arguments are also passed to the scroll helper that returns
results. Of special note here, an additional ``query`` keyword argument can be
passed to limit the records imported from the server. This query must follow the
Elasticsearch query of filter DSL. For more information on Elasticsearch query
DSL, please refer to `Elasticsearch's DSL
docs <https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html>`_.

.. code-block:: python

   >>> query = "{"filtered": {"query": {"match": { "tweet": "full text search"}}}}"
   >>> corpus = read_input(source="https://localhost/test_index", content_field="tweet",
                           query=query)


Output formats
^^^^^^^^^^^^^^

Output formats are how your data are represented to further processing and
modeling. To ensure a uniform interface, output formats implement the interface
described by ``topik.intermediaries.raw_data.CorpusInterface``. Presently, two such
backends are implemented: ``DictionaryCorpus`` and ``ElasticSearchCorpus``. Available
outputs can be examined by checking the keys of the ``topik.registered_outputs``
dictionary:

.. code-block:: python

    >>> from topik import registered_outputs
    >>> list(registered_outputs.keys())


The default output is the ``DictionaryCorpus``. No additional arguments are
necessary.  ``DictionaryCorpus`` stores everything in a Python dictionary. As such,
it is memory intensive. All operations done with a ``DictionaryCorpus`` block until
complete. ``DictionaryCorpus`` is the simplest to use, but it will ultimately limit
the size of analyses that you can perform.

The ``ElasticSearchCorpus`` can be specified to ``read_input`` using the ``output_type``
argument. It must be accompanied by another keyword argument, ``output_args``, which
should be a dictionary containing connection details and any additional
arguments.

.. code-block:: python

    >>> output_args = {"source": "localhost", "index": "destination_index"}
    >>> raw_data = read_input("test_data.json", output_format='elastic',
                              output_args=output_args, content_field="text")


``ElasticSearchCorpus`` stores everything in an Elasticsearch instance that you
specify. Operations do not block, and have "eventual consistency": the corpus
will eventually have all of the documents you sent available, but not
necessarily immediately after the read_input function returns. This lag time is
due to Elasticsearch indexing the data on the server side.


Tokenizing raw input
--------------------

The next step in topic modeling is to break your documents up into individual
terms. This is called tokenization. Tokenization is done using the ``tokenize``
method on a Corpus object (returned from ``read_input``):

.. code-block:: python

   >>> raw_data.tokenize()

The tokenize method accepts a few arguments to specify a tokenization method and
control behavior therein. The available tokenization methods are available in
the ``topik.tokenizers.tokenizer_methods`` dictionary. The presently available
methods are:

  * "simple": (default) lowercases input text and extracts single words. Uses
    Gensim.
  * "collocation": Collects bigrams and trigrams in addition to single words.
    Uses NLTK.
  * "entities": Extracts noun phrases as entities. Uses NLTK.
  * "mixed": first extracts noun phrases as entities, then follows up with
    simple tokenization for single words. Uses NLTK.

All methods accept a keyword argument ``stopwords``, which are words that will
be ignored in tokenization. These are words that add little content value, such
as prepositions. The default, STOPWORDS, uses gensim's STOPWORDS collection.


Collocation tokenization
~~~~~~~~~~~~~~~~~~~~~~~~

Collocation tokenization collects phrases of words (pairs and triplets, bigrams
and trigrams) that occur together often throughout your collection of documents.
There are two steps to tokenization with collocation: establishing the patterns
of bigrams and trigrams, and subsequently tokenizing each document individually.

To obtain the bigram and trigram patterns, use the
``topik.tokenizers.collect_bigrams_and_trigrams`` function:


.. code-block:: python

   >>> from topik.tokenizers import collect_bigrams_and_trigrams
   >>> patterns = collect_bigrams_and_trigrams(corpus)


Parameterization is done at this step, prior to tokenization of the corpus.  Tweakable parameters are:

  * top_n: limit results to a maximum number
  * min_word_length: the minimum length that any single word can be
  * min_bigram_freq: the minimum number of times a pair of words must occur together to be included
  * min_trigram_freq: the minimum number of times a triplet of words must occur together to be included


.. code-block:: python

   >>> patterns = collect_bigrams_and_trigrams(corpus, min_word_length=3, min_bigram_freq=3, min_trigram_freq=3)


For small bodies of text, you'll need small freq values, but this may be
correspondingly "noisy."

Next, feed the patterns into the ``tokenize`` method of your corpus object:

.. code-block:: python

   >>> raw_data.tokenize(method="collocation", patterns=patterns)
   

Proceed next to topic modeling!


Entities tokenization
~~~~~~~~~~~~~~~~~~~~~

We refer to entities as noun phrases, as extracted by `the TextBlob library
<https://textblob.readthedocs.org/en/dev/>`_. Like collocation tokenization,
entities tokenization is a two-step process. First, you establish noun phrases
using the ``topik.tokenizers.collect_entities`` function:

.. code-block:: python

   >>> from topik.tokenizers import collect_entities
   >>> entities = collect_entities(corpus)


You can tweak noun phrase extraction with a minimum and maximum occurrence frequency.  This is the frequency across your entire corpus of documents.

.. code-block:: python

   >>> entities = collect_entities(corpus, freq_min=4, freq_max=10000)


Next, tokenize the document collection:


.. code-block:: python

   >>> raw_data.tokenize(method="entities", entities=entities)


Proceed next to topic modeling!


Mixed tokenization
~~~~~~~~~~~~~~~~~~

Mixed tokenization employs both the entities tokenizer and the simple tokenizer, for when the entities tokenizer is overly restrictive, or for when words are interesting both together and apart.  Usage is similar to the entities tokenizer:

.. code-block:: python

   >>> from topik.tokenizers import collect_entities
   >>> entities = collect_entities(corpus)
   >>> raw_data.tokenize(method="mixed", entities=entities)


Proceed next to topic modeling!


Topic modeling
--------------

Topic modeling performs some mathematical modeling of your input data as a (sparse) matrix of which documents contain which words, attempting to identify latent "topics".  At the end of modeling, each document will have a mix of topics that it belongs to, each with a weight.  Each topic in turn will have weights associated with the collection of words from all documents.

Currently, Topik provides interfaces to or implements two topic modeling algorithms, LDA (latent dirichlet allocation) and PLSA (probablistic latent semantic analysis).  LDA is ___.  PLSA is ___.

Presently, all topic models require you to specify your desired number of topics as input to the modeling process.  With too many topics, and you will overfit your data, making your topics difficult to make sense of.  With too few, you'll merge topics together, which may hide important differences.  Make sure you play with the ntopics parameter to come up with the results that are best for your collection of data.

To perform topic modeling on your tokenized data, select a model class from the ``topik.models.registered_models`` dictionary, or simply import a model class directly, and instantiate this object with your corpus and the number of topics to model:

.. code-block:: python

   >>> from topik.models import registered_models, LDA
   >>> model = registered_models["LDA"](tokenized_data, 4)
   >>> model = LDA(tokenized_data, 4)


Viewing results
~~~~~~~~~~~~~~~

Each model supports a few standard outputs for examination of results:

  * List of top N words for each topic
  * Termite plots
  * LDAvis-based plots

Each model is free to implement its own additional outputs - check the class members for what might be available.


Saving and loading results
~~~~~~~~~~~~~~~~~~~~~~~~~~

The model object has a ``save`` method.  This method saves a JSON file that describes how to load the rest of the data for your model and for your corpus.  The ``topik.models.load_model`` function will read that JSON file, and recreate the necessary corpus and model objects to leave you where you saved.  Each model has its own binary representation, and each corpus type has its own storage backend.  The JSON file saved here generally does not include corpus data nor model data, but rather is simply instructions on where to find those data.  If you move files around on your hard disk, make sure to pick up everything with the JSON file.

.. code-block:: python

   >>> model.save("test_data.json")
   >>> from topik.models import load_model
   >>> model = load_model("test_data.json")
   >>> model.get_top_words(10)
