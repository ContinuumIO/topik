Introduction Tutorial
=====================

In this tutorial we will examine `topik` with a practical example: Topic
Modeling for Movie Reviews.

- The Movie Review Dataset
- Using the high-level interface ``run_topic_model``
- Creating your own custom topic modeling flow
- Analyzing the results


The Movie Review Dataset
------------------------

In this tutorial we are going to use the `Sentiment Polarity Dataset Version 2.0
<http://www.cs.cornell.edu/people/pabo/movie-review-data/>`_ from Bo Pang and
Lillian Lee. This dataset is distributed with `NLTK <http://www.nltk.org/>`_
with permission from the authors.

You can download the individual dataset from `NLTK
<http://www.nltk.org/nltk_data/packages/corpora/movie_reviews.zip>`_, or
download all of ntlk's dataset, running the following commands from the python
interpreter:

.. code-block:: python

   >>> import nltk
   >>> nltk.download()


For more information on the datasets and download options visit `NLTK data
<http://www.nltk.org/data.html>`_.

Instead of using the dataset in for `sentiment analysis`, its initial purpose,
we'll perform `topic modeling` on the movie reviews. For that reason, we'll
merge both folders `pos` and `neg`, to one named `reviews`.


High-level interfaces
---------------------


For quick, one-off studies, the command line interface allows you to specify
minimal information and obtain topic model plot output.


Custom topic modeling flow
--------------------------


For interactive exploration and more efficient, involved workflows, there also
exists a Python API for using each part of the topic modeling workflow. There
are three phases to topic modeling with topik: data import,
tokenization/vectorization, and modeling. Each phase is modular, with several
options available to you for each step.


Data Import
~~~~~~~~~~~

Data import loads your data from some external representation into an iterable,
internal representation for Topik. The main front end for importing data is the
read_input function:


.. code-block:: python

   >>> from topik import read_input
   >>> corpus = read_input(source="data_file.json", content_field="text")


``read_input`` is a front-end to several potential reader backends. Presently,
``read_input`` attempts to recognize which backend to use based on some
characteristics of the source string you pass in. These criteria are:

  * ends with .js or .json: treat as JSON stream filename first, fall back to
    "large JSON" (such as file generated by esdump).
  * contains 8983: treat as solr connection address (8983 is the default solr
    port).
  * contains 9200: treat as Elasticsearch connection address (9200 is the
    default Elasticsearch port).
  * result of os.path.splitext(source)[1] is "": treat as folder of files. Each
    file is considered raw text, and its contents are stored under the key given
    by content_field. Files may be gzipped.

Any of the backends can also be forced by passing the source_type argument with
one of the following string arguments:

  * solr
  * elastic
  * json_stream
  * large_json
  * folder

The ``content_field`` is a mandatory argument that in most cases specifies where the
actual content to be analyzed will be drawn from. For all hierarchical data
sources (everything except folders), this accesses some subfield of the data you
feed in.


JSON additional options
^^^^^^^^^^^^^^^^^^^^^^^

For JSON stream and "large JSON" inputs, an additional keyword argument may be
passed, ``json_prefix``, which is the period-separated path leading to the single
content field. This is for content fields not at the root level of the JSON
document. For example, given the JSON content:

.. code-block:: javascript

    [ {"nested": {"dictionary": {"text": "I am the text you're looking for."} } } ]


You would read using the following ``json_prefix`` argument:


.. code-block:: python

   >>> corpus = read_input(source="data_file.json", content_field="text", json_prefix="nested.dictionary")


Elasticsearch additional options and notes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The Elasticsearch importer expects a full string specifying the Elasticsearch
server. This string at a minimum must contain both the server address and the
index to access (if any). All results returned from the Elasticsearch query
contain only the contents of the '_source' field returned from the query.

.. code-block:: python

   >>> corpus = read_input(source="https://localhost/test_index", content_field="text")


Extra arguments passed by keyword are passed to the Elasticsearch instance
creation. This can be used to pass additional login parameters, for example, to
use SSL:

.. code-block:: python

   >>> corpus = read_input(source="https://user:secret@localhost:443/test_index", content_field="text", use_ssl=True)


The source argument for Elasticsearch also supports multiple servers, though
this requires that you manually specify the 'elastic' source_type:

.. code-block:: python

    >>> corpus = read_input(source=["https://server1", "https://server2"], content_field="text")


For more information on server options, please refer to 'Elasticsearch's
documentation<https://elasticsearch-py.readthedocs.org/en/master/>'_.

Extra keyword arguments are also passed to the scroll helper that returns
results. Of special note here, an additional ``query`` keyword argument can be
passed to limit the records imported from the server. This query must follow the
Elasticsearch query of filter DSL. For more information on Elasticsearch query
DSL, please refer to 'Elasticsearch's DSL
docs<https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html>'_.

.. code-block:: python

   >>> query = "{"filtered": {"query": {"match": { "tweet": "full text search"}}}}"
   >>> corpus = read_input(source="https://localhost/test_index", content_field="tweet", query=query)


Output formats
^^^^^^^^^^^^^^

Output formats are how your data are represented to further processing and
modeling. To ensure a uniform interface, output formats implement the interface
described by ``topik.intermediaries.raw_data.CorpusInterface``. Presently, two such
backends are implemented: ``DictionaryCorpus`` and ``ElasticSearchCorpus``. Available
outputs can be examined by checking the keys of the topik.registered_outputs
dictionary:

.. code-block:: python

    >>> from topik import registered_outputs >>> list(registered_outputs.keys())


The default output is the ``DictionaryCorpus``. No additional arguments are
necessary.  ``DictionaryCorpus`` stores everything in a Python dictionary. As such,
it is memory intensive. All operations done with a ``DictionaryCorpus`` block until
complete. ``DictionaryCorpus`` is the simplest to use, but it will ultimately limit
the size of analyses that you can perform.

The ``ElasticSearchCorpus`` can be specified to read_input using the output_type
argument. It must be accompanied by another keyword argument, output_args, which
should be a dictionary containing connection details and any additional
arguments.

.. code-block:: python

    >>> output_args = {} >>> raw_data = read_input("test_data.json",
    output_format='elastic', output_args=output_args)


``ElasticSearchCorpus`` stores everything in an Elasticsearch instance that you
specify. Operations do not block, and have "eventual consistency": the corpus
will eventually have all of the documents you sent available, but not
necessarily immediately after the read_input function returns. This lag time is
due to Elasticsearch indexing the data on the server side.


Analyzing the results
---------------------
